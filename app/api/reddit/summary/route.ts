import { NextRequest, NextResponse } from 'next/server';
import { auth } from '@/lib/auth';
import { callSnowflakeLLM } from '@/lib/snowflakeLLM';
import { redditDigestRequestSchema, redditDigestResponseSchema } from '@/lib/validators';
import { z } from 'zod';

type RedditSearchResponse = {
  data?: {
    children?: { data?: { permalink?: string } }[];
  };
};

export async function POST(req: NextRequest) {
  // Require authentication to prevent abuse
  const session = await auth();
  if (!session?.user?.id) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
  }

  try {
    const body = await req.json();
    const { searchTerm, topN, timeRange } = redditDigestRequestSchema.parse(body);

    // Build Reddit search URL
    const encodedQuery = encodeURIComponent(searchTerm.trim());
    const searchUrl = `https://www.reddit.com/search.json?q=${encodedQuery}&sort=relevance&t=${timeRange}&limit=${topN}`;

    // Fetch search results
    const searchRes = await fetch(searchUrl, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (compatible; RedditDigest/1.0)',
      },
    });

    if (!searchRes.ok) {
      throw new Error(`Reddit search failed with status ${searchRes.status}`);
    }

    const searchJson: RedditSearchResponse = await searchRes.json();
    const permalinks =
      searchJson.data?.children
        ?.map((c) => c.data?.permalink)
        .filter(Boolean) ?? [];

    if (permalinks.length === 0) {
      return NextResponse.json({
        summary: {
          bugs: [],
          features: [],
          suggestions: [],
          pros: [],
          cons: [],
          other: ['No posts found matching your search term.'],
        },
      });
    }

    // Fetch individual post JSON for each permalink
    const topNPermalinks = permalinks.slice(0, topN);
    const postJsonPromises = topNPermalinks.map(async (permalink) => {
      const trimmed = (permalink as string).endsWith('/')
        ? (permalink as string).slice(0, -1)
        : permalink;
      const url = `https://www.reddit.com${trimmed}.json`;

      const res = await fetch(url, {
        headers: {
          'User-Agent': 'Mozilla/5.0 (compatible; RedditDigest/1.0)',
        },
      });

      if (!res.ok) {
        console.error(`Failed to fetch post ${url}: ${res.status}`);
        return null;
      }
      return res.json();
    });

    const posts = (await Promise.all(postJsonPromises)).filter(Boolean);

    if (posts.length === 0) {
      return NextResponse.json({
        summary: {
          bugs: [],
          features: [],
          suggestions: [],
          pros: [],
          cons: [],
          other: ['Failed to fetch post details.'],
        },
      });
    }

    // Build LLM prompt with structured response
    const prompt = `You are analyzing Reddit posts generated by the search term: "${searchTerm}". Below is the raw JSON data from ${posts.length} Reddit post(s).

Your task is to extract actionable feedback and categorize it into:
- bugs: Technical issues, errors, or broken functionality mentioned
- features: New feature requests or ideas
- suggestions: General improvements, UX tweaks, or recommendations
- pros (optional): Positive feedback, things users like, or what's working well
- cons (optional): Negative feedback, pain points, or complaints
- other (optional): Any other notable feedback that doesn't fit the above

If there a post or comment mentions irrelevant information regarding the search term, ignore it.

Each item should be a concise sentence or phrase. Be specific and actionable.

Raw Reddit JSON:
${JSON.stringify(posts, null, 2)}

Respond ONLY with valid JSON in this exact format (no markdown, no code blocks, just the raw JSON):
{
  "bugs": ["bug description 1", "bug description 2"],
  "features": ["feature request 1", "feature request 2"],
  "suggestions": ["suggestion 1", "suggestion 2"],
  "pros": ["positive feedback 1"],
  "cons": ["negative feedback 1"],
  "other": ["other feedback 1"]
}`;

    // Call LLM without response_format (not supported by Snowflake)
    const llmResponse = await callSnowflakeLLM({
      prompt,
    });

    // Parse the LLM response
    let summary;
    try {
      let content = llmResponse?.choices?.[0]?.message?.content;
      if (!content) {
        throw new Error('No content in LLM response');
      }

      // Clean up the response - remove markdown code blocks if present
      content = content.trim();
      content = content.replace(/^```json\s*/i, '').replace(/^```\s*/, '').replace(/\s*```$/, '');
      content = content.trim();

      // Parse and validate with Zod
      const parsed = JSON.parse(content);
      summary = redditDigestResponseSchema.parse(parsed);
    } catch (parseError) {
      console.error('Failed to parse LLM response:', parseError);
      console.error('Raw content:', llmResponse?.choices?.[0]?.message?.content);
      return NextResponse.json(
        { error: 'Failed to parse AI summary result' },
        { status: 500 }
      );
    }

    return NextResponse.json({ summary });
  } catch (error) {
    if (error instanceof z.ZodError) {
      return NextResponse.json(
        { error: 'Invalid request data', details: error.errors },
        { status: 400 }
      );
    }

    console.error('Reddit digest API error:', error);
    return NextResponse.json(
      { error: 'Failed to generate Reddit digest' },
      { status: 500 }
    );
  }
}
